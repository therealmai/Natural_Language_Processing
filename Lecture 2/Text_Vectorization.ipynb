{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b736512-9858-4bda-97c9-c3abf00220bf",
   "metadata": {},
   "source": [
    "$\\textbf{Text Vectorization}$\n",
    "-\n",
    "\n",
    "- a vector is a geometric object which contains a magnitude and a direction.\n",
    "\n",
    "- Text vectorization is the projection of words into a mathematical space while preserving information.\n",
    "\n",
    "$\\textbf{The Bag of Words Model}$\n",
    "-\n",
    "\n",
    "- The BOW is a straight forward model for vectorizing sentences.\n",
    "\n",
    "- BOW uses word frequencies to construct vectors.\n",
    "\n",
    "- BOW model is an orderless document representation and only the counts of the words matter.\n",
    "\n",
    "- Because BOW does not take into account the positioning of words we loss smenatic information.\n",
    "\n",
    "- Vectorizing different sentences and joining the result into a single vocabulary.\n",
    "\n",
    "- The vocabulary acts as a reference if a specific word is present or absent in each of the sentence.\n",
    "\n",
    "$EXAMPLE$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15453f40-a0f4-4f3a-8e9f-b1d74636fd84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mat', 'love', 'cat', 'dog', 'sat']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "s1 = \"dog sat mat.\"\n",
    "s2 = \"cat love dog.\"\n",
    "\n",
    "def token_sentence(s):\n",
    "    # Make a regular expression that matches all punctuation\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # Use the regex\n",
    "    res = regex.sub('', s)\n",
    "    res = res.split()\n",
    "    return res\n",
    "\n",
    "new_s1 = token_sentence(s1)\n",
    "new_s2 = token_sentence(s2)\n",
    "vocabulary = list(set(new_s1 + new_s2))\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05ca5929-3e4f-4ce7-a40e-ed454af9812f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'sat', 'mat']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87c86c6b-4122-4186-b631-998bd84d378f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW = [int(u in new_s1) for u in vocabulary]\n",
    "BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07116216-ed5e-421d-95e7-fd69f1115685",
   "metadata": {},
   "source": [
    "$\\text{Term Frequency Inverse Document Frequency (TF-IDF)}$\n",
    "-\n",
    "\n",
    "- A model largely used in search engines to query relevant documents.\n",
    "\n",
    "- Two informations are encoded: the term frequency, and the inverse document frequency.\n",
    "\n",
    "- The term frequency is the count of words appearing in a document.\n",
    "\n",
    "- The inverse document frequency measures the importance of words in a document.\n",
    "\n",
    "- The inverse document frequency is calculated by logarithmically scaling the inverse fraction of the documents containing the word. This is obtained by dividing the total number of documents by the number of documents containing the term, followed by taking the logarithm of the ratio.\n",
    "\n",
    "- The inverse document frequency measures how common or rare a term is among all documents.\n",
    "\n",
    "The formula are:\n",
    "\\begin{gather}\n",
    "TF(t) = \\frac{\\text{number of times the term \"t\" appeas in a specific document}}{\\text{total number of terms in the document}}\n",
    "\\end{gather}\n",
    "\n",
    "\\begin{gather}\n",
    "IDF(t) = log(\\frac{\\text{total number of documents}}{\\text{number of documents with term \"t\"}})\n",
    "\\end{gather}\n",
    "\n",
    "\\begin{gather}\n",
    "TF \\cdotp IDF = TF(t) \\cdotp IDF(t)\n",
    "\\end{gather}\n",
    "\n",
    "- TF-IDF has more information that using vector representation because instead of using the count of words as used in the BOW, TF-IDF makes rare terms more prominent and ignores common words like stopwords such as \"is\", \"that\", \"of\", etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524507ac-0062-4b88-b0f3-e17f80564ee5",
   "metadata": {},
   "source": [
    "$\\text{Vectorization Using Gensim}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e0be93f-0819-40a8-9604-4da4916737f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "documents = [\"Harmful algal blooms (HABs) occur when algae or cyanobacteria grow excessively, causing harm to humans, animals, or the environment.\"\n",
    "             ,\"Factors that contribute to HABs in salt, brackish, and freshwater bodies include nutrient pollution (e.g., nitrogen and phosphorus run off from land-based sources) and warmer water temperatures.\"\n",
    "             ,\"Climate change effects might alter the occurrence and severity of HABs in the U.S., increasing the risk to human health and well-being.\"\n",
    "             ,\"Health effects in humans are usually associated with exposures to toxins produced during a HAB.\"\n",
    "             ,\"Illnesses caused by HABs encompass a range of symptoms and severity, dependent on factors such as types and concentrations of algae, cyanobacteria, and toxins involved as well as exposure routes and pre-existing conditions (e.g., asthma).\"\n",
    "             ,\"To diagnose HAB-associated illnesses, providers need a basic awareness of HABs and the ability to identify clinical presentations and exposures.\",\"HAB-associated illnesses are primarily a diagnosis of exclusion because clinical testing options for HAB toxins are lacking; ideally, providers have access to clinical diagnostic testing to rule out other possible causes. \"\n",
    "             ,\"Healthcare providers contribute to public health efforts when they identify and report HAB-associated illnesses to their jurisdictional health authority.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b506768-b958-45c7-b94a-e50ac8ea6571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['harmful', 'algal', 'bloom', 'hab', 'occur', 'algae', 'cyanobacteria', 'grow', 'excessively', 'cause', 'harm', 'human', 'animal', 'environment'], ['factor', 'contribute', 'hab', 'salt', 'brackish', 'freshwater', 'body', 'include', 'nutrient', 'pollution', 'e.g.', 'nitrogen', 'phosphoru', 'run', 'land', 'base', 'source', 'warm', 'water', 'temperature'], ['climate', 'change', 'effect', 'alter', 'occurrence', 'severity', 'hab', 'U.S.', 'increase', 'risk', 'human', 'health'], ['health', 'effect', 'human', 'usually', 'associate', 'exposure', 'toxin', 'produce', 'HAB'], ['illness', 'cause', 'hab', 'encompas', 'range', 'symptom', 'severity', 'dependent', 'factor', 'type', 'concentration', 'algae', 'cyanobacteria', 'toxin', 'involve', 'exposure', 'route', 'pre', 'existing', 'condition', 'e.g.', 'asthma'], ['diagnose', 'HAB', 'associate', 'illness', 'provider', 'need', 'basic', 'awareness', 'hab', 'ability', 'identify', 'clinical', 'presentation', 'exposure'], ['HAB', 'associate', 'illness', 'primarily', 'diagnosis', 'exclusion', 'clinical', 'testing', 'option', 'HAB', 'toxin', 'lack', 'ideally', 'provider', 'access', 'clinical', 'diagnostic', 'testing', 'rule', 'possible', 'cause'], ['healthcare', 'provider', 'contribute', 'public', 'health', 'effort', 'identify', 'report', 'HAB', 'associate', 'illness', 'jurisdictional', 'health', 'authority']]\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for document in documents:\n",
    "    text = []\n",
    "    doc = nlp(document)\n",
    "    for w in doc:\n",
    "        if not w.is_stop and not w.is_punct and not w.like_num:\n",
    "            text.append(w.lemma_)\n",
    "    texts.append(text)\n",
    "#texts is a mini-corpus specifically for toxic algal bloom\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce52e115-56cb-4821-9a51-2822e46126e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algae': 0, 'algal': 1, 'animal': 2, 'bloom': 3, 'cause': 4, 'cyanobacteria': 5, 'environment': 6, 'excessively': 7, 'grow': 8, 'hab': 9, 'harm': 10, 'harmful': 11, 'human': 12, 'occur': 13, 'base': 14, 'body': 15, 'brackish': 16, 'contribute': 17, 'e.g.': 18, 'factor': 19, 'freshwater': 20, 'include': 21, 'land': 22, 'nitrogen': 23, 'nutrient': 24, 'phosphoru': 25, 'pollution': 26, 'run': 27, 'salt': 28, 'source': 29, 'temperature': 30, 'warm': 31, 'water': 32, 'U.S.': 33, 'alter': 34, 'change': 35, 'climate': 36, 'effect': 37, 'health': 38, 'increase': 39, 'occurrence': 40, 'risk': 41, 'severity': 42, 'HAB': 43, 'associate': 44, 'exposure': 45, 'produce': 46, 'toxin': 47, 'usually': 48, 'asthma': 49, 'concentration': 50, 'condition': 51, 'dependent': 52, 'encompas': 53, 'existing': 54, 'illness': 55, 'involve': 56, 'pre': 57, 'range': 58, 'route': 59, 'symptom': 60, 'type': 61, 'ability': 62, 'awareness': 63, 'basic': 64, 'clinical': 65, 'diagnose': 66, 'identify': 67, 'need': 68, 'presentation': 69, 'provider': 70, 'access': 71, 'diagnosis': 72, 'diagnostic': 73, 'exclusion': 74, 'ideally': 75, 'lack': 76, 'option': 77, 'possible': 78, 'primarily': 79, 'rule': 80, 'testing': 81, 'authority': 82, 'effort': 83, 'healthcare': 84, 'jurisdictional': 85, 'public': 86, 'report': 87}\n"
     ]
    }
   ],
   "source": [
    "#creating a BOW representation of the mini-corpus\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301c05e0-56b6-4b11-b502-df817f675c57",
   "metadata": {},
   "source": [
    "$INSIGHTS$\n",
    "\n",
    "- There are 87 unique words in our corpus that is focused on healthcare and toxic algal bloom.\n",
    "\n",
    "- Each word is indexed with an integer.\n",
    "\n",
    "- The index is termed as a \"word ID\".\n",
    "\n",
    "- The BOW now can be used for word integer-id mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c5abbe-918d-45b7-8e38-8f0e00c2db1e",
   "metadata": {},
   "source": [
    "Using the doc2bow method, which, as the name suggests, helps convert our document to bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "801b1848-d1c6-435e-8ee8-c7c50c572f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 1),\n",
       "  (2, 1),\n",
       "  (3, 1),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1)],\n",
       " [(9, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1),\n",
       "  (30, 1),\n",
       "  (31, 1),\n",
       "  (32, 1)],\n",
       " [(9, 1),\n",
       "  (12, 1),\n",
       "  (33, 1),\n",
       "  (34, 1),\n",
       "  (35, 1),\n",
       "  (36, 1),\n",
       "  (37, 1),\n",
       "  (38, 1),\n",
       "  (39, 1),\n",
       "  (40, 1),\n",
       "  (41, 1),\n",
       "  (42, 1)],\n",
       " [(12, 1),\n",
       "  (37, 1),\n",
       "  (38, 1),\n",
       "  (43, 1),\n",
       "  (44, 1),\n",
       "  (45, 1),\n",
       "  (46, 1),\n",
       "  (47, 1),\n",
       "  (48, 1)],\n",
       " [(0, 1),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (9, 1),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (42, 1),\n",
       "  (45, 1),\n",
       "  (47, 1),\n",
       "  (49, 1),\n",
       "  (50, 1),\n",
       "  (51, 1),\n",
       "  (52, 1),\n",
       "  (53, 1),\n",
       "  (54, 1),\n",
       "  (55, 1),\n",
       "  (56, 1),\n",
       "  (57, 1),\n",
       "  (58, 1),\n",
       "  (59, 1),\n",
       "  (60, 1),\n",
       "  (61, 1)],\n",
       " [(9, 1),\n",
       "  (43, 1),\n",
       "  (44, 1),\n",
       "  (45, 1),\n",
       "  (55, 1),\n",
       "  (62, 1),\n",
       "  (63, 1),\n",
       "  (64, 1),\n",
       "  (65, 1),\n",
       "  (66, 1),\n",
       "  (67, 1),\n",
       "  (68, 1),\n",
       "  (69, 1),\n",
       "  (70, 1)],\n",
       " [(4, 1),\n",
       "  (43, 2),\n",
       "  (44, 1),\n",
       "  (47, 1),\n",
       "  (55, 1),\n",
       "  (65, 2),\n",
       "  (70, 1),\n",
       "  (71, 1),\n",
       "  (72, 1),\n",
       "  (73, 1),\n",
       "  (74, 1),\n",
       "  (75, 1),\n",
       "  (76, 1),\n",
       "  (77, 1),\n",
       "  (78, 1),\n",
       "  (79, 1),\n",
       "  (80, 1),\n",
       "  (81, 2)],\n",
       " [(17, 1),\n",
       "  (38, 2),\n",
       "  (43, 1),\n",
       "  (44, 1),\n",
       "  (55, 1),\n",
       "  (67, 1),\n",
       "  (70, 1),\n",
       "  (82, 1),\n",
       "  (83, 1),\n",
       "  (84, 1),\n",
       "  (85, 1),\n",
       "  (86, 1),\n",
       "  (87, 1)]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872bb77c-6c24-4c81-87b6-267220cfabcb",
   "metadata": {},
   "source": [
    "- The output is a nested list.\n",
    "\n",
    "- Each individual sublist represents a documents bag-of-words representation.\n",
    "\n",
    "- A reminder: you might see different numbers in your list, this is because each time you create a dictionary, different mappings will occur.\n",
    "\n",
    "- Unlike the example we demonstrated, where an absence of a word was a 0, we use tuples that represent (word_id, word_count).\n",
    "\n",
    "- We can easily verify this by checking the original sentence, mapping each word to its integer ID and reconstructing our list.\n",
    "\n",
    "- We can also notice in this case each document has not greater than one count of each word - in smaller corpuses, this tends to happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd28cb55-4fb5-46fe-8405-dc84d12c7a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing your generated corpus\n",
    "\n",
    "corpora.MmCorpus.serialize('/tmp/example.mm', corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56ae242-51ef-4169-8f5b-f1609372b15b",
   "metadata": {},
   "source": [
    "- It is more memory efficient to store your corpus into the disk and later loading it because at most one vector resides in the RAM at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b091d55-bb06-406c-8832-031cbec45f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.20687441490900804), (1, 0.31031162236351206), (2, 0.31031162236351206), (3, 0.31031162236351206), (4, 0.14636752736880834), (5, 0.20687441490900804), (6, 0.31031162236351206), (7, 0.31031162236351206), (8, 0.31031162236351206), (9, 0.07013786431820669), (10, 0.31031162236351206), (11, 0.31031162236351206), (12, 0.14636752736880834), (13, 0.31031162236351206)]\n",
      "[(9, 0.05420937381791797), (14, 0.2398390498522999), (15, 0.2398390498522999), (16, 0.2398390498522999), (17, 0.15989269990153326), (18, 0.15989269990153326), (19, 0.15989269990153326), (20, 0.2398390498522999), (21, 0.2398390498522999), (22, 0.2398390498522999), (23, 0.2398390498522999), (24, 0.2398390498522999), (25, 0.2398390498522999), (26, 0.2398390498522999), (27, 0.2398390498522999), (28, 0.2398390498522999), (29, 0.2398390498522999), (30, 0.2398390498522999), (31, 0.2398390498522999), (32, 0.2398390498522999)]\n",
      "[(9, 0.07805568921230585), (12, 0.1628908769625549), (33, 0.34534253059492104), (34, 0.34534253059492104), (35, 0.34534253059492104), (36, 0.34534253059492104), (37, 0.23022835372994738), (38, 0.1628908769625549), (39, 0.34534253059492104), (40, 0.34534253059492104), (41, 0.34534253059492104), (42, 0.23022835372994738)]\n",
      "[(12, 0.2501092157670636), (37, 0.3535018907902139), (38, 0.2501092157670636), (43, 0.17675094539510694), (44, 0.17675094539510694), (45, 0.2501092157670636), (46, 0.5302528361853208), (47, 0.2501092157670636), (48, 0.5302528361853208)]\n",
      "[(0, 0.17183578052203283), (4, 0.12157703657826278), (5, 0.17183578052203283), (9, 0.058258507532545946), (18, 0.17183578052203283), (19, 0.17183578052203283), (42, 0.17183578052203283), (45, 0.12157703657826278), (47, 0.12157703657826278), (49, 0.2577536707830492), (50, 0.2577536707830492), (51, 0.2577536707830492), (52, 0.2577536707830492), (53, 0.2577536707830492), (54, 0.2577536707830492), (55, 0.08591789026101641), (56, 0.2577536707830492), (57, 0.2577536707830492), (58, 0.2577536707830492), (59, 0.2577536707830492), (60, 0.2577536707830492), (61, 0.2577536707830492)]\n",
      "[(9, 0.08135691446371011), (43, 0.11998272432507807), (44, 0.11998272432507807), (45, 0.16978005418562137), (55, 0.11998272432507807), (62, 0.3599481729752342), (63, 0.3599481729752342), (64, 0.3599481729752342), (65, 0.23996544865015615), (66, 0.3599481729752342), (67, 0.23996544865015615), (68, 0.3599481729752342), (69, 0.3599481729752342), (70, 0.16978005418562137)]\n",
      "[(4, 0.11402438220709378), (43, 0.16116093356565447), (44, 0.08058046678282724), (47, 0.11402438220709378), (55, 0.08058046678282724), (65, 0.32232186713130895), (70, 0.11402438220709378), (71, 0.24174140034848168), (72, 0.24174140034848168), (73, 0.24174140034848168), (74, 0.24174140034848168), (75, 0.24174140034848168), (76, 0.24174140034848168), (77, 0.24174140034848168), (78, 0.24174140034848168), (79, 0.24174140034848168), (80, 0.24174140034848168), (81, 0.48348280069696337)]\n",
      "[(17, 0.23092216476345012), (38, 0.3267635225549296), (43, 0.11546108238172506), (44, 0.11546108238172506), (55, 0.11546108238172506), (67, 0.23092216476345012), (70, 0.1633817612774648), (82, 0.34638324714517515), (83, 0.34638324714517515), (84, 0.34638324714517515), (85, 0.34638324714517515), (86, 0.34638324714517515), (87, 0.34638324714517515)]\n"
     ]
    }
   ],
   "source": [
    "#Converting Bag-of-Words to TF-IDF representation\n",
    "from gensim import models\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "for document in tfidf[corpus]:\n",
    "       print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b163e098-a7bb-4b74-9feb-d699bc2de793",
   "metadata": {},
   "source": [
    "- TF-IDF scores: The higher the score, the more important the word in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c612c4-f77c-4992-acb1-54bf9a20b26a",
   "metadata": {},
   "source": [
    "$\\textbf{N-Gramming}$\n",
    "-\n",
    "\n",
    "- Context is very important when working with text data.\n",
    "- This context is lost during vector representation because on only the word frequency is taken into account.\n",
    "- An n-gram is a contiguous sequence of n items in the text. In our case, we will be dealing with words being the item, but depending on the use case, it could be even letters, syllables, or sometimes in the case of speech, phonemes.\n",
    "- Mono-gram, n=1\n",
    "- Bi-gram, n = 2.\n",
    "- Tri-gram, n=3\n",
    "- N-Gramming is calculated through the conditional probability of a token given by thr preceding token.\n",
    "- N-Gramming can also be done by calculating words that appear close to each other.\n",
    "- Bi-gramming is also called co-location, it locates pair of words that are very likely to appear close together.\n",
    "- Example: \"New Hampshire\" is one word not \"New\" and \"Hampshire\"\n",
    "- Gensim approaches bigrams by simply combining the two high probability tokens with an underscore. The tokens new and york will now become new_york instead. Similar to the TF- IDF model, bigrams can be created using another Gensim model - Phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a3eb89c2-8315-4b7e-82c9-a2b09daa78ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['harmful',\n",
       "  'algal',\n",
       "  'bloom',\n",
       "  'hab',\n",
       "  'occur',\n",
       "  'algae',\n",
       "  'cyanobacteria',\n",
       "  'grow',\n",
       "  'excessively',\n",
       "  'cause',\n",
       "  'harm',\n",
       "  'human',\n",
       "  'animal',\n",
       "  'environment'],\n",
       " ['factor',\n",
       "  'contribute',\n",
       "  'hab',\n",
       "  'salt',\n",
       "  'brackish',\n",
       "  'freshwater',\n",
       "  'body',\n",
       "  'include',\n",
       "  'nutrient',\n",
       "  'pollution',\n",
       "  'e.g.',\n",
       "  'nitrogen',\n",
       "  'phosphoru',\n",
       "  'run',\n",
       "  'land',\n",
       "  'base',\n",
       "  'source',\n",
       "  'warm',\n",
       "  'water',\n",
       "  'temperature'],\n",
       " ['climate',\n",
       "  'change',\n",
       "  'effect',\n",
       "  'alter',\n",
       "  'occurrence',\n",
       "  'severity',\n",
       "  'hab',\n",
       "  'U.S.',\n",
       "  'increase',\n",
       "  'risk',\n",
       "  'human',\n",
       "  'health'],\n",
       " ['health',\n",
       "  'effect',\n",
       "  'human',\n",
       "  'usually',\n",
       "  'associate',\n",
       "  'exposure',\n",
       "  'toxin',\n",
       "  'produce',\n",
       "  'HAB'],\n",
       " ['illness',\n",
       "  'cause',\n",
       "  'hab',\n",
       "  'encompas',\n",
       "  'range',\n",
       "  'symptom',\n",
       "  'severity',\n",
       "  'dependent',\n",
       "  'factor',\n",
       "  'type',\n",
       "  'concentration',\n",
       "  'algae',\n",
       "  'cyanobacteria',\n",
       "  'toxin',\n",
       "  'involve',\n",
       "  'exposure',\n",
       "  'route',\n",
       "  'pre',\n",
       "  'existing',\n",
       "  'condition',\n",
       "  'e.g.',\n",
       "  'asthma'],\n",
       " ['diagnose',\n",
       "  'HAB',\n",
       "  'associate',\n",
       "  'illness',\n",
       "  'provider',\n",
       "  'need',\n",
       "  'basic',\n",
       "  'awareness',\n",
       "  'hab',\n",
       "  'ability',\n",
       "  'identify',\n",
       "  'clinical',\n",
       "  'presentation',\n",
       "  'exposure'],\n",
       " ['HAB',\n",
       "  'associate',\n",
       "  'illness',\n",
       "  'primarily',\n",
       "  'diagnosis',\n",
       "  'exclusion',\n",
       "  'clinical',\n",
       "  'testing',\n",
       "  'option',\n",
       "  'HAB',\n",
       "  'toxin',\n",
       "  'lack',\n",
       "  'ideally',\n",
       "  'provider',\n",
       "  'access',\n",
       "  'clinical',\n",
       "  'diagnostic',\n",
       "  'testing',\n",
       "  'rule',\n",
       "  'possible',\n",
       "  'cause'],\n",
       " ['healthcare',\n",
       "  'provider',\n",
       "  'contribute',\n",
       "  'public',\n",
       "  'health',\n",
       "  'effort',\n",
       "  'identify',\n",
       "  'report',\n",
       "  'HAB',\n",
       "  'associate',\n",
       "  'illness',\n",
       "  'jurisdictional',\n",
       "  'health',\n",
       "  'authority']]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "bigram = gensim.models.Phrases(texts)\n",
    "texts = [bigram[line] for line in texts]\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c48145-2d7d-4bef-9c83-622cce63c0ab",
   "metadata": {},
   "source": [
    "$\\textbf{NOTE}:$Since by creating new phrases we add words to our dictionary, this step must be done before we create our dictionary. We would have to run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "afbd5839-6714-4498-a6f3-c2086ba06d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06f30b2-849e-4c3f-92ab-959c9048d8b3",
   "metadata": {},
   "source": [
    "After we are done creating our bi-grams, we can create tri-grams, and other n-grams by simply running the phrases model multiple times on our corpus. Bi-grams still remains the most used n-gram model, though it is worth one's time to glance over the other uses and kinds of n-gram implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d3c80adf-ac0e-40a7-985d-168183113ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing both high frequency and low-frequency words.\n",
    "# Example: get rid of words that occur in less than 20 documents, or in more than 50% of the documents, \n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8048d733-c179-4a97-b02a-e7572cf7c80b",
   "metadata": {},
   "source": [
    "$\\textbf{Programming Assignment}$\n",
    "\n",
    "Choose a topic that you will be using as a term paper for this subject. Collect articles, publications, sotries etc. of your chosen topic and develop your own mini-corpus using the preprocessing steps required. Be sure to print the output.\n",
    "\n",
    "Note that this corpus will be used for the entire subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f91a849-ac61-4038-b37e-db8b9cbcd6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
